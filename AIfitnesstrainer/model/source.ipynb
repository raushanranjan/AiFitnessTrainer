{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "\n",
    "# importing drawing tools\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "\n",
    "# importing the pose estimation models\n",
    "mp_pose = mp.solutions.pose \n",
    "\n",
    "# importing media on which prediction must be made\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# counter\n",
    "counter = 0\n",
    "stage = None\n",
    "\n",
    "# setting up mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        # ret is return and frame is the video\n",
    "        # if ret == false then media wasn't found\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # recolor image to RGB and pass it to Mediapipe because OpenCV's default format is BGR\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # making detections\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # recolor back to openCV's default format BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # rendering detections on video\n",
    "        # passing three parameters - image, the landmarks and which landmark connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), # landmark/joints color\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) # connections/bones color\n",
    "                                 )\n",
    "        # extracting landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # extracting landmarks of hip,knee and ankle (both left and right side)\n",
    "            right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "            right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "            left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            \n",
    "            # calculating angle at both knees\n",
    "            angleAtLeftKnee = round(calculate_angle(left_hip, left_knee, left_ankle),2)\n",
    "            angleAtRightKnee = round(calculate_angle(right_hip, right_knee, right_ankle),2)\n",
    "            meanKnee = round((angleAtLeftKnee + angleAtRightKnee)/2,2)\n",
    "            \n",
    "            \n",
    "            # calculating angle at hip\n",
    "            angleAtLeftHip = round(calculate_angle(left_shoulder, left_hip, left_knee),2)\n",
    "            angleAtRightHip = round(calculate_angle(right_shoulder, right_hip, right_knee),2)\n",
    "            meanHip = round((angleAtLeftHip + angleAtRightHip)/2,2)\n",
    "            \n",
    "            # distance between both knees\n",
    "            distance = calculate_distance(right_knee,left_knee)\n",
    "            \n",
    "            # creating a list which has angle at left and right knee, mean of these angles and distance between them respectively\n",
    "            features = [meanHip,meanKnee,distance]\n",
    "            features = list(np.array(features))\n",
    "            \n",
    "            # Make Detections\n",
    "            X = pd.DataFrame([features])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "                           \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # squat counter logic\n",
    "        if meanKnee < 120:\n",
    "            stage = \"down\"\n",
    "        if meanKnee > 160 and stage =='down':\n",
    "            stage=\"up\"\n",
    "            counter +=1\n",
    "            print(counter)\n",
    "        \n",
    "        screen_res = 1280, 720\n",
    "        scale_width = screen_res[0] / image.shape[1]\n",
    "        scale_height = screen_res[1] / image.shape[0]\n",
    "        scale = min(scale_width, scale_height)\n",
    "        #resized window width and height\n",
    "        window_width = int(image.shape[1] * scale)\n",
    "        window_height = int(image.shape[0] * scale)\n",
    "        #cv2.WINDOW_NORMAL makes the output window resizealbe\n",
    "        cv2.namedWindow('Resized Window', cv2.WINDOW_NORMAL)\n",
    "        #resize the window according to the screen resolution\n",
    "        cv2.resizeWindow('Resized Window', window_width, window_height)\n",
    "        \n",
    "        # Get status box\n",
    "        cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "        # Display Class\n",
    "        cv2.putText(image, 'FORM',\n",
    "                    (130,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, body_language_class.split(' ')[0],\n",
    "                    (80,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display Rep\n",
    "        cv2.putText(image, 'REPS', (15,12), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), \n",
    "                    (25,42), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        cv2.imshow('Resized Window', image)\n",
    "    \n",
    "        \n",
    "        \n",
    "        # to check if tried to close our screen or we pressed the button q\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
